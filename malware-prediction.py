# -*- coding: utf-8 -*-
"""ml_assignment_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v46U1llNy3VjFuf-31Du2ZQkmTKDzfTl

# Microsoft Malware Prediction

Created by: 
1. Vaibhavi Tikone [MT2020006]
2. Hritik Arora [MT2020003]
3. Souvik Das [MT2020159]

## Table Of content:
 

1.   Import libraries
2.   Load the data
3.   Statistics
      3.1 Missing values
      3.2 Skewness
      3.3 Unique values
4.   EDA
      4.1 Target class distribution
      4.2 Plots of features
      4.3 Correlation Matrix
5.   Feature Engineering
      5.1 Feature Selection
      5.2 Split dependent features
6.   Imputation
7.   Encoding
8.   Model Training
9.   Feature Importance
10.  KDE Plot
11.  Submission file

### 1. Import libraries
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import matplotlib.pyplot as plt
import seaborn as sns
from plotly.offline import iplot, init_notebook_mode #for offline graphs
import plotly.graph_objects as go #graph

import lightgbm as lgb #decision tree based model
from sklearn.metrics import roc_auc_score #Compute (ROC AUC) from prediction scores
from sklearn.model_selection import KFold #K-Folds cross-validator

import re #regular expression
import gc
import os

from google.colab import drive
drive.mount('/content/drive')

"""### 2. Load the Data"""

# you can give path here to load the data
train=pd.read_csv("/content/drive/MyDrive/malware detection/train.csv")
test=pd.read_csv("/content/drive/MyDrive/malware detection/test.csv")

print(train.shape,test.shape)

"""### 3. Statistics"""

insights = []
for col in train.columns:
    insights.append((col, train[col].nunique(), train[col].isnull().sum() * 100 / train.shape[0],train[col].value_counts(normalize=True, dropna=False).values[0] * 100, train[col].dtype))
    
table = pd.DataFrame(insights, columns=['Features', 'Unique_values', 'missing_values_percent', 'skewness', 'type'])

"""#### 3.1 missing values

"""

table1=table.sort_values('missing_values_percent', ascending=False)
table1.head(10)

"""#### 3.2 Skewness

"""

table2=table.sort_values('skewness', ascending=False)
table2.head(10)

"""#### 3.3 Unique values"""

table3=table.sort_values('Unique_values', ascending=True)
table3.head(10)

"""### 4. EDA

#### 4.1 Target Class distribution
"""

cnt_srs = train['HasDetections'].value_counts()
labels = (np.array(cnt_srs.index))
sizes = (np.array((cnt_srs / cnt_srs.sum())*100))

trace = go.Pie(labels=labels, values=sizes)
layout = go.Layout(
    title='Target distribution',
    font=dict(size=14),
    width=400,
    height=350,
)
data = [trace]
fig = go.Figure(data=data, layout=layout)
iplot(fig, filename="usertype")

plt.title('Target class Distribution')
train['HasDetections'].value_counts().plot.bar()

"""#### 4.2 Plots features
From above table we get that we can drop some of features like PuaMode, IsBeta, AutoSampleOptIn, Census_IsWIMBootEnabled. But to have close look at other features which are there in the table, bt lesser skewed than above listed 2-4 features. So, let's have a close look of distribution of those features.
"""

def plot_func(X,Y):
  plt.figure(figsize=(14, 8))
  plt.subplot(231)
  plt.title('train')
  sns.countplot(x=X, hue=Y, data=train)
  plt.subplot(232)
  plt.title('test')
  sns.countplot(x=X, data=test)
  plt.suptitle('Useless')
  plt.show()

plot_func("Census_DeviceFamily","HasDetections")

plot_func("Census_InternalBatteryType","HasDetections")

plot_func("Census_IsPortableOperatingSystem","HasDetections")

plot_func("Census_IsFlightingInternal","HasDetections")

plot_func("Census_IsFlightsDisabled","HasDetections")

plot_func("Census_ThresholdOptIn","HasDetections")

plot_func("Census_IsVirtualDevice","HasDetections")

"""We can see that data are differently distributed among categories, and for target class, too. So, will not drop this columns.

#### 4.3 Correlation matrix
"""

def corr_fun(train):
  corrMatrix = train.corr()
  plt.title('Correlation HeatMap')
  sns.heatmap(corrMatrix, annot=True)
  plt.figure(figsize=(30, 15))
  plt.show()

corr_fun(train)

"""### 5. Feature Engineering

#### 5.1 Feature Selection

We will select those features only which are not highly skewed, not highly null valued and which has uniques values greater than 1. So, we will simply drop rest of the columns.
"""

train=train.drop(['IsBeta','AutoSampleOptIn','PuaMode','Census_ProcessorClass'],axis=1)
test=test.drop(['IsBeta','AutoSampleOptIn','PuaMode','Census_ProcessorClass'],axis=1)

"""#### 5.2 Split dependent features"""

target = train['HasDetections']
machine_id = test['MachineIdentifier']
train.drop(['MachineIdentifier', 'HasDetections'], inplace=True, axis=1)
test.drop(['MachineIdentifier'], inplace=True, axis=1)

"""### 6. Impution

After having close look at the dataset, we can say that almost all the features are categorical regardless of its datatype. So, for imputation we will simply replace null values of each feature by its own mode value.
"""

#for all the features
for feature in train.columns:
    train[feature].fillna((train[feature].mode()[0]),inplace=True)
    test[feature].fillna((test[feature].mode()[0]),inplace=True)

"""### 7. Encoding

##### Frequency Encoding
"""

def frequency_encoding(cat_names):
  for i in cat_names:
    fe=train.groupby(i).size()/len(train)
    train1.loc[:,i]=train[i].map(fe)
    fe=test.groupby(i).size()/len(test)
    test1.loc[:,i]=test[i].map(fe)

#for encoding purpose
cat_names = train.select_dtypes(include='object').columns
#Let's have different of dataset for encoding and training purpose
train1=train.copy()
test1=test.copy()
#call function
frequency_encoding(cat_names)

train1.head()

gc.collect()

"""### 8. Model Training

#### LightGBM
"""

def model(train1,test1):
    # light gbm model cannot handle the json characters, so we need to perform following
    train1 = train1.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))
    test1 = test1.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))
    
    #for model prediction and ploting the graph
    train_predictions = np.zeros(train1.shape[0], dtype='float32')
    test_predictions = np.zeros(test1.shape[0], dtype='float32')
    
    kf = KFold(n_splits=5, shuffle=True)
    clf = lgb.LGBMClassifier(max_depth=-1, n_estimators=6000, scale_pos_weight=1.667,learning_rate=0.01, num_leaves=250, colsample_bytree=0.28, objective='binary', n_jobs=-1)
  
    for (train_index, test_index) in kf.split(train1, target):
        x_train, y_train = train1.iloc[train_index], target.iloc[train_index]
        x_test, y_test = train1.iloc[test_index], target.iloc[test_index]

        clf.fit(x_train, y_train, eval_metric='auc', eval_set=[(x_test, y_test)], verbose=200, early_stopping_rounds=100)
    clf_pred=clf.predict_proba(test1)[:,1]
    clf_pred_train=clf.predict_proba(train1)[:,1]
    train_predictions = np.add(train_predictions, clf_pred_train).astype('float32')
    test_predictions = np.add(test_predictions, clf_pred).astype('float32')
    yield train_predictions,test_predictions
    feat_imp=pd.Series(clf.feature_importances_, index=train.columns)
    yield feat_imp
    yield clf_pred

iterator = model(train1,test1)
train_prediction,test_predition=next(iterator)

"""### 9. Feature Importance"""

feat_imp = next(iterator)
feat_imp.nlargest(20).plot(kind='barh', figsize=(15,10))

"""### 10. KDE(kernel density estimation) plot"""

# plot the kernel densities of predicted train probabilities
plt.title("train")
sns.kdeplot(train_prediction)
plt.show()

# plot the kernel densities of predicted test probabilities
plt.title("test")
sns.kdeplot(test_predition)
plt.show()

"""### 11. Submission file"""

clf_pred=next(iterator)
submit=pd.DataFrame({'MachineIdentifier':machine_id,'HasDetections':clf_pred})

submit.describe

#submit.to_csv("./_submission_.csv",index=False)
